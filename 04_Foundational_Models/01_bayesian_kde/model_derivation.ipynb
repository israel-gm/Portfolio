{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba129a",
   "metadata": {},
   "source": [
    "# Model derivation\n",
    "\n",
    "This classifier models:\n",
    "\n",
    "$P(y|x) \\propto P(x|y)P(y)$\n",
    "\n",
    "where\n",
    "\n",
    "- $P(x|y)$ is estimated non-parametrically using KDE, one KDE per class.\n",
    "\n",
    "- $P(y)$ is the class prior, estimated from class frequencies.\n",
    "\n",
    "Let's step through this code and discuss the essential features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "class bayesian_kde_classifier(BaseEstimator, ClassifierMixin):\n",
    "\t\"\"\"Bayesian generative classification based on KDE\n",
    "\tParameters\n",
    "\t----------\n",
    "\tbandwidth : float\n",
    "\tthe kernel bandwidth within each class\n",
    "\tkernel : str\n",
    "\tthe kernel name, passed to KernelDensity\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "\t\tself.bandwidth = bandwidth\n",
    "\t\tself.kernel = kernel\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\tself.classes_ = np.sort(np.unique(y))\n",
    "\t\ttraining_sets = [X[y == yi] for yi in self.classes_]\n",
    "\t\tself.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "\t\t\t\t\t\tkernel=self.kernel).fit(Xi)\n",
    "\t\t\t\tfor Xi in training_sets]\n",
    "\t\tself.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "\t\t\t\t\t\tfor Xi in training_sets]\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict_proba(self, X):\n",
    "\t\tlogprobs = np.array([model.score_samples(X)\n",
    "\t\t\t\tfor model in self.models_]).T\n",
    "\t\tresult = np.exp(logprobs + self.logpriors_)\n",
    "\t\treturn result / result.sum(1, keepdims=True)\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\treturn self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5d25a",
   "metadata": {},
   "source": [
    "The general approach for Bayesian generative classification is this:\n",
    "\n",
    "1. Split the training data by label.\n",
    "\n",
    "2. For each set, fit a KDE to obtain a generative model of the data. This allows you for any observation $x$ and label $y$ to compute a likelihood $P(x|y)$\n",
    "\n",
    "3. From the number of examples of each class in the training set, compute the class prior, $P(y)$.\n",
    "\n",
    "\n",
    "4. For an unknown point $x$, the posterior probability for each class is $P(y|x) \\propto P(x|y)P(y)$. The class which maximizes this posterior is the label assigned to the point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c5a84",
   "metadata": {},
   "source": [
    "## Anatomy of the custom estimator\n",
    "\n",
    "\n",
    " 1. Each estimator in Scikit-Learn is a class, and it is most convenient for this class to inherit from the BaseEstimator class as well as the appropriate mixin, which provides standard functionality. For example, among other things, here the BaseEstimator contains the logic necessary to clone/copy an estimator for use in a cross-validation procedure, and ClassifierMixin defines a default score() method used by such routines. We also provide a doc string, which will be captured by IPython's help functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Bayesian generative classification based on KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bandwidth : float\n",
    "        the kernel bandwidth within each class\n",
    "    kernel : str\n",
    "        the kernel name, passed to KernelDensity\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed8507",
   "metadata": {},
   "source": [
    "2. This is the actual code that is executed when the object is instantiated with KDEClassifier(). In Scikit-Learn, it is important that initialization contains no operations other than assigning the passed values by name to self. This is due to the logic contained in BaseEstimator required for cloning and modifying estimators for cross-validation, grid search, and other functions. Similarly, all arguments to __ init __ should be explicit: i.e. *args or **kwargs should be avoided, as they will not be correctly handled within cross-validation routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79381a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c90da",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Here we find the unique classes in the training data, train a KernelDensity model for each class, and compute the class priors based on the number of input samples. Finally, fit() should always return self so that we can chain commands. For example:\n",
    "\n",
    "`label = model.fit(X, y).predict(X)`\n",
    "\n",
    "\n",
    "Why logs?\n",
    "\n",
    "KDE outputs log-likelihoods, adding logs avoids underflow:\n",
    "\n",
    "$logP(x,y)=logP(x|y)+logP(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y)) # Extracts unique class labels from y\n",
    "        training_sets = [X[y == yi] for yi in self.classes_] # Splits X into subsets for each class\n",
    "        self.models_ = [KernelDensity(bandwidth=self.bandwidth, # Initializes a KDE model for each class,\n",
    "                                      kernel=self.kernel).fit(Xi) # Fits the model to the class-specific data, this is P(x|y).\n",
    "                        for Xi in training_sets]\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0]) # Computes the log prior probabilities for each class, this is P(y).\n",
    "                           for Xi in training_sets]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6887da",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Purpose: Compute posterior class probabilities $P(y|x)$. Because this is a probabilistic classifier, we first implement predict_proba() which returns an array of class probabilities of shape `[n_samples, n_classes]`. Entry `[i, j]` of this array is the posterior probability that sample i is a member of class j.\n",
    "\n",
    "This is Bayesâ€™ rule in vectorized form\n",
    "\n",
    "$P(y|x)=\\frac{P(x|y)P(y)}{\\sum_{c'} P(x|c')P(c')}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c62dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "def predict_proba(self, X):\n",
    "\t\tlogprobs = np.array([model.score_samples(X) # Computes log P(x|y)\n",
    "\t\t\t\tfor model in self.models_]).T # Transposes to shape (n_samples, n_classes)\n",
    "\t\tresult = np.exp(logprobs + self.logpriors_) # Computes P(x|y) * P(y)\n",
    "\t\treturn result / result.sum(1, keepdims=True) # Normalizes across classes to get P(y|x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fec0d",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Finally, the `predict()` method uses these probabilities and simply returns the class with the largest probability.\n",
    "\n",
    "$\\hat{y}=arg \\max_{y}P(y|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea90f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "def predict(self, X):\n",
    "\t\treturn self.classes_[np.argmax(self.predict_proba(X), 1)] # Predicts the class with the highest posterior probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_314_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
