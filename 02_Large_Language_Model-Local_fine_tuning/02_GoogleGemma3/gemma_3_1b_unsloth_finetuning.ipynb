{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0b4db1",
   "metadata": {},
   "source": [
    "# Gemma 3 1B: How to Run & Fine-tune\n",
    "\n",
    "Googleâ€™s Gemma 3 1B (only text input) fine-tune locally using unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2f3e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32965853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-21 17:48:05 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953819ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.6: Fast Gemma3 patching. Transformers: 4.53.2. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f6ecbc",
   "metadata": {},
   "source": [
    "Let's first experience how Gemma 3 1B can handle text input. We use Gemma 3 1B recommended settings of `temperature = 1.0`, `top_p = 0.95`, `top_k = 64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5832ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    _ = model.generate(\n",
    "        **tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\"),\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a8ee4",
   "metadata": {},
   "source": [
    "Let's make a poem about sloths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c006d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write a poem about sloths.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here's a poem about sloths, aiming for a peaceful, slightly whimsical tone:\n",
      "\n",
      "**The Slow Dance of the Green**\n",
      "\n",
      "A velvet hush, a shadowed grace,\n",
      "A sloth descends, a peaceful space.\n",
      "Upon a branch, so thick and high,\n",
      "A languid rhythm, passing by.\n",
      "\n",
      "With slow, deliberate, gentle pace,\n",
      "He hangs suspended, a leafy face.\n",
      "A mossy cloak, a dappled hue,\n",
      "He dreams of ferns, and morning dew.\n",
      "\n",
      "No frantic chase, no hurried stride,\n",
      "Just breath held deep, with nowhere to hide.\n",
      "A tiny\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{ \"type\" : \"text\",\n",
    "                  \"text\" : \"Write a poem about sloths.\" }]\n",
    "}]\n",
    "do_gemma_3n_inference(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c544ca",
   "metadata": {},
   "source": [
    "## Gemma 3 1B finetune.\n",
    "\n",
    "LoRA adapters so we only need to update a small amount of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62e1e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87bcb3",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "We now use the Gemma-3 format for conversation style finetunes. We use Maxime Labonne's FineTome-100k dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed654c7",
   "metadata": {},
   "source": [
    "We use `get_chat_template` function to get the correct chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49b5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36359e55",
   "metadata": {},
   "source": [
    "Get the first 3000 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a793bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train[:3000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbfc8c",
   "metadata": {},
   "source": [
    "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a06c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18f867",
   "metadata": {},
   "source": [
    "Let's see how row 100 looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38cc674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'What is the modulus operator in programming and how can I use it to calculate the modulus of two given numbers?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'In programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.',\n",
       "   'role': 'assistant'}],\n",
       " 'source': 'infini-instruct-top-500k',\n",
       " 'score': 4.774171352386475}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88853b97",
   "metadata": {},
   "source": [
    "We now have to apply the chat template for Gemma-3 onto the conversations, and save it to text. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cc545",
   "metadata": {},
   "source": [
    "Let's see how the chat template did. Notice there is no `<bos>` token as the processor tokenizer will be adding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0258b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nWhat is the modulus operator in programming and how can I use it to calculate the modulus of two given numbers?<end_of_turn>\\n<start_of_turn>model\\nIn programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.<end_of_turn>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e34314",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now let's use Huggingface TRL's SFTTrainer. More docs here: [TRL SFT docs.](https://huggingface.co/docs/trl/sft_trainer) We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96643b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        dataset_num_proc=2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfd9e1",
   "metadata": {},
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9573fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d3b7a",
   "metadata": {},
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again. Notice how the sample only has a single `<bos>` as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "639f4233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWhat is the modulus operator in programming and how can I use it to calculate the modulus of two given numbers?<end_of_turn>\\n<start_of_turn>model\\nIn programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.<end_of_turn>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a057c7",
   "metadata": {},
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15380e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                               In programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.<end_of_turn>\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883dcf10",
   "metadata": {},
   "source": [
    "Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2d570fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4060 Laptop GPU. Max memory = 7.996 GB.\n",
      "2.471 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351eb7fc",
   "metadata": {},
   "source": [
    "## Let's train the model.\n",
    "\n",
    "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cca70071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,000 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 6,522,880 of 1,006,408,832 (0.65% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.814500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.047200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d2246",
   "metadata": {},
   "source": [
    "Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb538a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.5069 seconds used for training.\n",
      "1.39 minutes used for training.\n",
      "Peak reserved memory = 2.471 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 30.903 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdf654",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's run the model via Unsloth native inference. According to the Gemma-3 team, the recommended settings for inference are `temperature = 1.0`, `top_p = 0.95`, `top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d76011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final text type: <class 'str'>\n",
      "Final text: <bos><start_of_turn>user\n",
      "Continue the sequence: 1, 1, 2, 3, 5, 8,<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "['user\\nContinue the sequence: 1, 1, 2, 3, 5, 8,\\nmodel\\nThe sequence is the Fibonacci sequence where each term is the sum of the two preceding terms. In this case, 1 + 1 = 2, 1 + 2 = 3, 2 + 3 = 5, 3 + 5 = 8, 5 + 8 =']\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n",
    "    }]\n",
    "}]\n",
    "\n",
    "# Try different approaches to get the formatted text\n",
    "try:\n",
    "    # First attempt: with tokenize=False\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "except:\n",
    "    # Second attempt: without tokenize parameter\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "# Handle different return types\n",
    "if isinstance(text, list):\n",
    "    if len(text) > 0 and isinstance(text[0], str):\n",
    "        text = text[0]  # Take first string if it's a list of strings\n",
    "    else:\n",
    "        # If it's a list of tokens, decode them\n",
    "        text = tokenizer.decode(text, skip_special_tokens=True)\n",
    "elif not isinstance(text, str):\n",
    "    # If it's tokens, decode them\n",
    "    text = tokenizer.decode(text, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Final text type: {type(text)}\")\n",
    "print(f\"Final text: {text}\")\n",
    "\n",
    "# Now tokenize the string\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=1.0, \n",
    "    top_p=0.95, \n",
    "    top_k=64,\n",
    ")\n",
    "\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdd1ed",
   "metadata": {},
   "source": [
    "We can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f39947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is blue because of a phenomenon called Rayleigh scattering. This is the scattering of electromagnetic radiation by particles, in this case light particles of a certain wavelength. When the sun is lower in the sky and the sun is behind the observer, it is a bright bright blue because the light waves that are refracted are shorter in\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
    "}]\n",
    "\n",
    "# Apply chat template with tokenize=False to ensure string output\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,  # This ensures we get a string, not tokens\n",
    ")\n",
    "\n",
    "# Tokenize the string\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=1.0, \n",
    "    top_p=0.95, \n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bcb51",
   "metadata": {},
   "source": [
    "## Saving, loading finetuned models\n",
    "\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "[NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-3n-8bit/tokenizer_config.json',\n",
       " 'gemma-3n-8bit/special_tokens_map.json',\n",
       " 'gemma-3n-8bit/chat_template.jinja',\n",
       " 'gemma-3n-8bit/tokenizer.model',\n",
       " 'gemma-3n-8bit/added_tokens.json',\n",
       " 'gemma-3n-8bit/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3-1b-8bit\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3-1b-8bit\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557b5c9",
   "metadata": {},
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69009c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma-3 is a powerful open-source large language model developed by the Gemma team at Google DeepMind.  It's designed to be a generative language model that can be downloaded and run on devices with limited resources, as opposed to massive, proprietary large language models that require substantial computational resources. This means it\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastModel\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"gemma-3\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3?\",}]\n",
    "}]\n",
    "\n",
    "# Apply chat template with tokenize=False to ensure string output\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,  # This ensures we get a string, not tokens\n",
    ")\n",
    "\n",
    "# Tokenize the string\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    temperature=1.0, \n",
    "    top_p=0.95, \n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dafe6c",
   "metadata": {},
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to float16 directly for deployment. We save it in the folder gemma-3-1B-finetune. Set if `False` to if `True` to let it run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfc18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Successfully copied all 1 files from cache to gemma-3n-8bit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.83s/it]\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-1b-8bit\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0a5a8",
   "metadata": {},
   "source": [
    "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_ACCOUNT/gemma-3N-finetune\", tokenizer,\n",
    "        token = \"hf_...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de62ec1",
   "metadata": {},
   "source": [
    "## GGUF / llama.cpp Conversion\n",
    "\n",
    "To save to GGUF / llama.cpp, we support it natively now for all models! For now, you can convert easily to Q8_0, F16 or BF16 precision. Q4_K_M for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdaae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Updating system package directories\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3n-8bit\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {1152, 262144}\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {1152}\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_sep_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b87a926a4634fdb87dd5337f3a234a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma-3n-8bit.Q8_0.gguf with size = 1.1G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma-3n-8bit.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3-1b-8bit\",\n",
    "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444ba4d",
   "metadata": {},
   "source": [
    "Now, use the gemma-3N-finetune.gguf file or gemma-3N-finetune-Q4_K_M.gguf file in llama.cpp or a UI based system like Jan or Open WebUI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
